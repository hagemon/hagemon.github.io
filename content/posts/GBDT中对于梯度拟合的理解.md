+++
title = 'GBDT中对于梯度拟合的理解'
date = 2019-08-20 20:05:27
math = true
tags = ['Algorithm']
+++

为什么在优化GBDT时，要让当前的学习器去拟合一个负梯度呢？

在梯度提升决策树（Gradient Boosting Decision Tree, GBDT）中，当前学习器的拟合对象是**自定义损失函数$L(y_i,\hat{y}_i^{t-1})$对于上一个学习器的输出$\hat{y}_i^{t-1}$的负梯度**，即

$$-\frac{\partial{L(y_i,\hat{y}_i^{t-1})}}{\partial{\hat{y}_i^{t-1}}}$$

在自定义损失函数为均方误差函数时，这个负梯度恰好是学习器和标签之间的残差

$$-\frac{\partial{(y_i-\hat{y}_i^{t-1})^2}}{\partial{\hat{y}_i^{t-1}}}=2(y_i-\hat{y}_i^{t-1})$$

很多文章并没有对**为什么要拟合负梯度**这个问题给出一个详细的解释，很多人解释说是为了能够应对自定义的损失函数才使用了负梯度来代替残差。但是实际上，恰恰是因为使用了负梯度来拟合当前学习器的输出，才导致了在特殊情况下（使用均方误差作为损失函数的情况），学习器拟合的是残差。

所以为什么使用的是负梯度，而不是别的什么值呢？个人觉得这和梯度下降的原理十分的类似，我们从梯度下降开始说起。

## 梯度下降法的原理

在我们最早接触的高等数学里，假设有一个固定参数$\theta$的函数$f(x;\theta )=\theta_1 x^2+\theta_2x$，我们要求得这个函数的最小值，则要对$f(x)$关于变量$x$求导，找到导数为0的点，即在**变量空间中找到一个值使得函数最小**。

后来我们接触了机器学习，我们要求解的目标从自变量$x$变成了我们模型的参数$\theta$，目的也从在变量空间中找$x$变成了在**参数空间中找到一个$\theta$，使得对于所有的训练数据$x$，我们的目标函数（也就是损失函数）值能最小**。

我们经常使用的一个方法就是梯度下降法，它的形式为：

$$\theta_{t}=\theta_{t-1}-\eta\frac{\partial f(\theta_{t-1})}{\partial \theta_{t-1}}$$

我在这里增加了一个时间下标，主要是为了区分更新前后的$\theta$。

其实梯度下降是由泰勒一阶展开推导而来的，或者也可以从导数的定义来解释（这里以单变量为例，所以是导数，推广到多变量就可以变成梯度）

假设当前的参数为$\theta_{t-1}$，我们要求解的下一个参数为$\theta_{t}$，那么当两者之间的距离很小的时候，就可以通过导数的概念得出以下的式子：

$$\frac{f(\theta_{t})-f(\theta_{t-1})}{\theta_{t}-\theta_{t-1}}= f’(\theta_{t-1})$$

整理一下，可得

$$f(\theta_{t})-f(\theta_{t-1})=(\theta_{t}-\theta_{t-1})f'(\theta_{t-1})$$

其实这个公式整理一下就是泰勒一阶展开，这里的$(\theta_{t}-\theta_{t-1})$是一个向量，表示我们的参数向量，我们可以把它改写成**标量*单位向量**的形式：

$$(\theta_{t}-\theta_{t-1})=\eta e$$

我们优化的目标是让更新参数后的函数值比更新前的函数值小，即

$$f(\theta_{t})-f(\theta_{t-1})=\eta ef'(\theta_{t-1})<0$$

在不考虑标量$\eta$时，要使$ef'(\theta_t)$最小，则需要让单位向量$e$和导数的方向相反，那么则有

$$e=-\frac{f'(\theta_{t-1})}{||f'(\theta_{t-1})||}$$

其中分母因为是标量，可以并入到$\eta$中，则有

$\theta_{t}-\theta_{t-1}=\eta f'(\theta_{t-1})$

即

$$\theta_{t}=\theta_{t-1}-\eta\frac{\partial f(\theta_{}t-1)}{\partial \theta_{t-1}}$$

## GBDT

现在考虑GBDT里的情况。在梯度下降法中，我们的目的是为了在参数空间中找到一个参数，使得损失函数最小；而在GBDT中，我们的目的是为了在函数空间（也就是所有可能的树序列组成的空间）中，找到一个函数（某一个树序列）使得损失函数（“标签”与“树序列输出之和”的差异）最小。所以我们可以把GBDT和梯度下降做一个类比。

$f(\theta_{t})$可以类比为加入当前树之后的损失函数$l(y_i,\hat{y}_i^{t-1}+T_t(x))$ ，$f(\theta_{t-1})$类比为之前的损失函数$l(y_i,\hat{y}_i^{t-1})$，那么梯度下降中的参数$\theta$就可以类比为GBDT中累加的学习器$F_m$,$\theta_{t}$和$\theta_{t-1}$分别类比为加入当前树和没加入当前树的两个学习器$F_m(x)$和$F_{m-1}(x)$，且有

$$F_{m-1}(x)-F_m(x)=T_t(x)$$

这次我们从[泰勒展开](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F)的角度来讲，关于$\hat{y}_i^{t-1}$的函数$l(y_i,\hat{y}_i^{t-1}+T_t(x))$在$T_t(x)$处的泰勒一阶展开为：

$$l(y_i,\hat{y}_i^{t-1}+T_t(x))=l(y_i,\hat{y}_i^{t-1})+(F_{m-1}(x)-F_m(x))\frac{\partial l(y_i,\hat{y}_i^{t-1}+T_t(x))}{\partial \hat{y}_i^{t-1}}$$

这个式子其实也可以类比上一节中的式子：

$$f(\theta_{t})-f(\theta_{t-1})=(\theta_{t}-\theta_{t-1})f'(\theta_{t-1})$$

其实二者是一样的形式，所以我们也可以把最终梯度下降的表达式改写成GBDT的形式，有兴趣的话可以自己推一遍，其实形式上是一样的，只是换了个空间而已。

$$F_{m}=F_{m-1}-\eta\frac{\partial l(y_i,\hat{y}_i^{t-1}+T_t(x))}{\partial \hat{y}_i^{t-1}}$$

即

$$T_t(x)=-\eta\frac{\partial l(y_i,\hat{y}_i^{t-1}+T_t(x))}{\partial \hat{y}_i^{t-1}}$$

这也就是为什么我们要让新的树去拟合负梯度了。

## Ending

这么一总结，大胆猜测XGBoost使用了泰勒二阶展开，可能就是受了GBDT中使用一阶展开的启发。终于把很多博客都没有说的很清楚的部分记录下来了，当然这里也参考了一些写的很棒的博客和知乎回答，感谢大家的辛勤付出！

### 参考资料

[梯度下降法的推导](https://blog.csdn.net/pengchengliu/article/details/80932232)

[gbdt的残差为什么用负梯度代替？ - 奥奥奥奥噢利的回答 - 知乎](https://www.zhihu.com/question/63560633/answer/581670747)